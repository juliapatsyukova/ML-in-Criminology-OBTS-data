# -*- coding: utf-8 -*-
"""Data pre-processing (R)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9KKtbS9V7_nBU9py_6HQRRSuipcEun3

Long-to-wide tables
"""

install.packages('haven')
library(haven)
library("data.table")
library(data.table)
library(stats)

# Load the data table
dt_work <- setDT(read_xpt('06191-0001-Data.xpt'))

# Filter and assign NA values
dt_work[dt_work$V17 != '8', c(23:61) := .(NA)][dt_work$V27 == 4 | dt_work$V27 == 5| dt_work$V27 == 7|dt_work$V27 == 8 |dt_work$V27 == 9, c(33:61) := .(NA)][, c("V5","V9",'V12','V20','V23','V30','V33', 'V41', 'V51'):=NULL]

# Create a copy to work with
dt_z <- copy(dt_work)

# Set presence of information to 1 and absence to 0
dt_z[V1 != 99, V1 := 1][V1 == 99, V1 := 0][V2 != 9, V2 := 1][V2 == 9, V2 := 0][V3 != 9, V3 := 1][V3 == 9, V3 := 0][V4 != 9, V4 := 1][V4 == 9, V4 := 0][V10 != 9999, V10 := 1][V10 == 9999, V10 := 0][V11 != 99, V11 := 1][V11 == 99, V11 := 0][V13 != 99, V13 := 1][V13 == 99, V13 := 0][V15 != 9999, V15 := 1][V15 == 9999, V15 := 0][V16 != 9, V16 := 1][V16 == 9, V16 := 0][V17 != 99, V17 := 1][V17 == 99, V17 := 0][V18 != 9999, V18 := 1][V18 == 9999, V18 := 0][V19 != 99, V19 := 1][V19 == 99, V19 := 0][V21 != 99999, V21 := 1][V21 == 99999, V21 := 0][V22 != 9, V22 := 1][V22 == 9, V22 := 0][V24 != 9999, V24 := 1][V24 == 9999, V24 := 0][V25 != 9, V25 := 1][V25 == 9, V25 := 0][V26 != 99, V26 := 1][V26 == 99, V26 := 0][V27 != 9, V27 := 1][V27 == 9, V27 := 0][V28 != 99, V28 := 1][V28 == 99, V28 := 0][V29 != 99, V29 := 1][V29 == 99, V29 := 0][V31 != 99999, V31 := 1][V31 == 99999, V31 := 0][V32 != 9, V32 := 1][V32 == 9, V32 := 0][V34 != 9999, V34 := 1][V34 == 9999, V34 := 0][V35 != 9, V35 := 1][V35 == 9, V35 := 0][V37 != 99, V37 := 1][V37 == 99, V37 := 0][V38 != 99, V38 := 1][V38 == 99, V38 := 0][V39 != 9999, V39 := 1][V39 == 9999, V39 := 0][V40 != 99, V40 := 1][V40 == 99, V40 := 0][V42 != 99999, V42 := 1][V42 == 99999, V42 := 0][V43 != 9, V43 := 1][V43 == 9, V43 := 0][V44 != 9, V44 := 1][V44 == 9, V44 := 0][V45 != 9, V45 := 1][V45 == 9, V45 := 0][V46 != 9, V46 := 1][V46 == 9, V46 := 0][V47 != 9, V47 := 1][V47 == 9, V47 := 0][V48 != 9, V48 := 1][V48 == 9, V48 := 0][V49 != 9999, V49 := 1][V49 == 9999, V49 := 0][V50 != 99, V50 := 1][V50 == 99, V50 := 0][V52 != 99999, V52 := 1][V52 == 99999, V52 := 0][V53 != 99, V53 := 1][V53 == 99, V53 := 0][V54 != 999, V54 := 1][V54 == 999, V54 := 0][V55 != 99, V55 := 1][V55 == 99, V55 := 0][V56 != 999, V56 := 1][V56 == 999, V56 := 0][V57 != 999, V57 := 1][V57 == 999, V57 := 0][V58 != 99, V58 := 1][V58 == 99, V58 := 0][V59 != 999, V59 := 1][V59 == 999, V59 := 0][V60 != 9, V60 := 1][V60 == 9, V60 := 0][V61 != 9999, V61 := 1][V61 == 9999, V61 := 0][V14 != 88, V14 := 1][V14 == 88, V14 := 0][V36 != 99, V36 := 1][V36 == 99, V36 := 0]

# Remove well-filled columns
dt_z[, c('V6', "V7", "V10", "V14", "V16", "V17", "V18", "V22", "V27", "V28", 'V32', "V37", "V39", "V47", "V49", "V53", "V61"):=NULL]

# Save all column names except the municipality column into a vector
column <- colnames(dt_z)[-5]

column <- colnames(dt_z)[-5]  # All column names except county

# Write a loop
for (i in column) {
  # Convert to the l2w format
  l2w <- dcast(dt_z, formula = paste(i, "~ V8"), value.var = i)
  # Check and remove NA values in most columns, which means three rows need to be checked and removed
  if (nrow(l2w) == 3) {
    l2w <- l2w[-1, ]
  }
  # For pairwise, convert to a matrix, transpose it, remove the first row (where we have 1 and 0 as names for the calculated filled and missing values), swap the columns of the matrix to have the count of filled values first
  mat <- data.matrix(l2w)
  mat <- t(mat)
  mat <- mat[-1,]
  mat <- mat[,2:1]
  # Assign a name to each matrix
  assign(paste("mat_", i, sep = ""), mat, envir = .GlobalEnv)
}

"""Renaming counties"""

library(rJava)
library(remotes)
library(tidyr)
library(dplyr)
library(data.table)

# Load the tabulizer library
remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")
library(tabulizer)

# Pseudocode!
pdf_path <- "path to the codebook"
dt_ideal <- "path to the original dataset"

# Extract pages from the PDF file.
pdf_tables <- extract_tables(pdf_path, pages = 72:81, stream = TRUE, guess = FALSE)

# The tabulizer couldn't split most of the pages into separate columns, so we apply a loop, iterating over the data frames
# in the list and using a regular expression to split the main set of "code-county" pairs
for (i in 1:10) {
  pdf_data <- as.data.frame(pdf_tables[[i]], stringsAsFactors = FALSE)
  pdf_data_i <- separate(pdf_data, col = V1, into = c("county_1", "county_2"), sep = "(?<=[^0-9])(?=[0-9]{3})")

  assign(paste0("pdf_data_", i), pdf_data_i) # save the data frame as a separate object
}

# !!! Important! Here, I manually cleaned and standardized the data frames.
# First, I manually fixed the pairs that didn't split correctly (there are only a few of them), and second, I made the column names the same.

# Now we merge two columns into one following the following logic: column 1 row 1, column 2 row 1, and so on,
# so that each county is in its own list. This is important because later we assign a state code to each county based on its position in the row.
# Keep an eye on the column names!

for (i in 1:10) {
  new_col <- c(rbind(get(paste0('pdf_data_', i))$county_1,
                     get(paste0('pdf_data_', i))$county_2))
  new_df <- data.frame(new_col)
  assign(paste0("col", i), new_df)
}

# Combine all the lists into one big list
code_name <- rbind(col1, col2, col3, col4, col5, col6, col7, col8, col9, col10)

# Clean from NA and empty strings
df <- code_name[!(is.na(code_name$new_col) | code_name$new_col == ""), ]
df <- as.data.table(df)

# Split the codes and counties by the double space separator. Tabulizer will do most of the work for you,
# you just need to keep an eye on the separator during manual editing.

sep_cod_name <- separate(df, col = df, into = c("code", "county"), sep = "  ")

# Create a column and assign the state code
code_name_state <- sep_cod_name |>
  mutate(state = case_when(
    row_number() <= 70 ~ "AL", # +
    row_number() <= 77 ~ "AK", # +
    row_number() <= 137 ~ "CA", # +
    row_number() <= 142 ~ "DE",
    row_number() <= 187 ~ "ID", # +
    row_number() <= 235 ~ "KY",
    row_number() <= 329 ~ "MN", # +
    row_number() <= 446 ~ "MO", # +
    row_number() <= 541 ~ "NB", # +
    row_number() <= 569 ~ "NJ", # +
    row_number() <= 635 ~ "NY", # +
    row_number() <= 673 ~ "OR",
    row_number() <= 746 ~ "PA", # +
    row_number() <= 780 ~ "UT",
    row_number() <= 796 ~ "VT", # +
    row_number() <= 932 ~ "VA", # +
  ))

# Check which states are present in our dataset
unique(dt_ideal$V7)

# Remove unnecessary triples since there are more states in the codebook than in the dataset

code_name_state <- code_name_state[!(code_name_state$state == "DE" |
                                     code_name_state$state == "KY" |
                                     code_name_state$state == "OR" |
                                     code_name_state$state == "UT"),]

# Remove state names - we don't need them
code_name_state <- slice(code_name_state, c(-1, -71, -78, -138, -183,
                                            -277, -394, -489, -517, -583,
                                            -656, -672, -773))

# Some counties did not match correctly:
# "MN-C0N": replace O with 0 in code_name_state
# "NB-NSP": NSP county is in NJ - replace it in the original dataset
# "VA-MWA": in the original dataset, MWA county does not match the codebook (MFA). The correct version is likely from the dataset as it refers to metro Washington...
# "VT-VSP": there are two VSP, one from Vermont and one from Virginia. But both were added to Virginia in the codebook, so it needs to be corrected in code_name_state.
# "VT-DLC": in the codebook, DLC is located in Virginia, correct it in code_name_state.
# "VT-DFW": the same problem with DFW.

# C0N
code_name_state[270, 4] <- "MN-C0N"
code_name_state[270, 1] <- "C0N"

# MWA
code_name_state[793, 4] <- "VA-MWA"
code_name_state[793, 1] <- "MWA"

# VT-VSP
code_name_state[759, 3] <- "VT"
code_name_state[759, 4] <- "VT-VSP"

# DLC
code_name_state[760, 3] <- "VT"
code_name_state[760, 4] <- "VT-DLC"

# DFW
code_name_state[758, 3] <- "VT"
code_name_state[758, 4] <- "VT-DFW"

# NJ-NSP
dt_ideal[dt_ideal$state_code == "NB-NSP", "V7"][dt_ideal[dt_ideal$state_code == "NB-NSP", "V7"] == "NB"] <- "NJ"

# Some counties in the codebook are listed as unknown. This means that we are unaware of which district these data originated from.
# We standardize them to a consistent designation, and later, concatenation with the state can be done for differentiation.
code_name_state <- data.frame(lapply(code_name_state, function(x) gsub("COUNTY UNKNOWN", "Unknown", x)))

# Now we have two datasets with common values. We can create a key column 'state-code'.
# Creating key columns
code_name_state$state_code <- paste(code_name_state$state, code_name_state$code, sep = "-")

dt_ideal$state_code <- paste(dt_ideal$V7, dt_ideal$V8, sep = "-")

# Renaming V8 (county name) based on matching
# We take the name from our dictionary (code_name_state). If the key from the original dataset (dt_ideal) matches the key from code_name_state,
# we assign the county name from code_name_state to the cells in the V8 column.
dt_ideal$V8 <- code_name_state$county[match(dt_ideal$state_code, code_name_state$state_code)]

# Checking the number of unknown counties
x <- dt_ideal[dt_ideal$V8 == 'Unknown']
unique(x$V7)

# Saving the dataset
dt_ideal
write.table(dt_ideal, "MLcrim.csv", sep = ",", row.names = FALSE)